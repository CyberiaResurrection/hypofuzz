
Scille is building Parsec; it's basically Dropbox with everything encrypted.
https://parsec.cloud/en/blog/our_experience_with_hypothesis_testing/

This makes it harder because all the logic is on the user side - decentralising
means you don't have an authoritative server; have to do key management, etc.
Key revocation and reencryption if a laptop is stolen, some people working offline
on shared documents, etc.

Lots of of tricky integrated problems; writing tests by hand just won't work.
So we need a tool to write automatic tests,



What's the best thing about Hypothesis?

Easy: stateful testing.  OK, encode-decode is nice if you need it, but most
of the interesting problems are stateful.  Eg a shopping website, the filesystem
part of parsec - lots of operations but also invariants.

(idea: maybe we want to say "this is the rule (or subset) to focus on")
May or may not be useful - have tests for "going offline", "going online",
etc.  Potentially allows all of these to be tested together.
On the other hand it's like mashing everything into a single test -> bad!

Tools to suggest when a state machine is getting too large/complicated
could be great - much like the existing healthchecks.



What plugins do you wish for, or were easier to write?

Just hypothesis-trio!  Works beautifully :-)



What's your biggest pain point / wishlist?

when running multiple times; sometimes nondeterministic
adding print() etc (ie source code in hash)
the workarounds are fine so this is just annoying though.

there was an issue with inconsistency in the CI
All tests OK locally, then pushing to CI and it would randomly fail



Sharing the database - redis?

Some concern about operating system versions, changes to the code
invalidating things, etc.  But maybe useful?
We would also need some kind admin interface
How large would it get?
Do you have to be online to run this?  No - multiplexed DB



Fuzzing workflow: would it be useful?

Yes!

But: how would it fit into our CI workflow?  not in CI, should
be running at all times?
Just on master?  On CI server, or as a separate server?
Yeah, like Sentry - ala reporting issues from users.

Would this give us a false sense of security?  Like "it must have
found all the bugs so my code must be perfect" but the test is just bad.

How would this cope with huge ever-growing test suites?
I wonder about being an orchestrator - "this one never fails so run it later".

Sees dashboad: "ooh"

Would you be interested?  Is anything missing?

Thinking about how it could fit in our test system - we're really happy at the moment
Hypothesis covers lots of our testing process, so how much complexity added for
how much extra performance?
Hypothesis is already so good that it's hard to improve on!

TODO: harder sell when we get back in touch next week.


